import torch
from transformers import AutoImageProcessor, AutoModel
from PIL import Image
import time
import os
import numpy as np

# --- é…ç½® ---
MODEL_NAME = "facebook/dinov3-vith16plus-pretrain-lvd1689m"
TEST_IMAGE = "./source/test/recv08R6gTefu1.png" # è¯·ç¡®ä¿è¿™ä¸ªæ–‡ä»¶å­˜åœ¨
NUM_ITERATIONS = 50 # æ¨¡æ‹Ÿ50æ¬¡æœç´¢è¯·æ±‚

def benchmark():
    print(f"ğŸš€ æ­£åœ¨åŠ è½½ DINOv3 æ¨¡å‹ ({MODEL_NAME})...")
    start_load = time.time()
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    processor = AutoImageProcessor.from_pretrained(MODEL_NAME)
    # ä½¿ç”¨ bfloat16 ä»¥è·å¾—æœ€ä½³æ€§èƒ½å’Œç¨³å®šæ€§
    model = AutoModel.from_pretrained(MODEL_NAME, device_map="auto", torch_dtype=torch.bfloat16)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_load:.2f} ç§’")

    if not os.path.exists(TEST_IMAGE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æµ‹è¯•å›¾ç‰‡ {TEST_IMAGE}")
        return

    img = Image.open(TEST_IMAGE).convert('RGB')

    # --- 1. GPU é¢„çƒ­ (Warm-up) ---
    # ç¬¬ä¸€æ¬¡è¿è¡Œé€šå¸¸è¾ƒæ…¢ï¼Œå› ä¸ºéœ€è¦åˆå§‹åŒ– CUDA æ ¸å‡½æ•°
    print("ğŸ”¥ æ­£åœ¨é¢„çƒ­ GPU...")
    with torch.inference_mode():
        for _ in range(5):
            inputs = processor(images=img, return_tensors="pt").to(device).to(torch.bfloat16)
            _ = model(**inputs)
    print("âœ… é¢„çƒ­å®Œæˆ")

    # --- 2. æ€§èƒ½æµ‹è¯• (Profiling) ---
    print(f"â±ï¸ å¼€å§‹æµ‹è¯•å•å¼ å›¾ç‰‡æ¨ç†é€Ÿåº¦ (å¾ªç¯ {NUM_ITERATIONS} æ¬¡)...")
    latencies = []

    with torch.inference_mode():
        for i in range(NUM_ITERATIONS):
            start_time = time.time()
            
            # æ­¥éª¤ 1: é¢„å¤„ç†
            inputs = processor(images=img, return_tensors="pt").to(device).to(torch.bfloat16)
            
            # æ­¥éª¤ 2: æ¨ç†
            outputs = model(**inputs)
            embedding = outputs.pooler_output
            
            # æ­¥éª¤ 3: å¼ºåˆ¶åŒæ­¥ CUDAï¼ˆç¡®ä¿æˆ‘ä»¬æµ‹é‡çš„æ˜¯ GPU çœŸå®æ‰§è¡Œæ—¶é—´ï¼‰
            if device == "cuda":
                torch.cuda.synchronize()
            
            end_time = time.time()
            latency = (end_time - start_time) * 1000 # è½¬æ¢ä¸ºæ¯«ç§’
            latencies.append(latency)
            
            if (i+1) % 10 == 0:
                print(f"å·²å®Œæˆ {i+1}/{NUM_ITERATIONS}...")

    # --- 3. ç»Ÿè®¡ç»“æœ ---
    avg_latency = np.mean(latencies)
    p95_latency = np.percentile(latencies, 95)
    min_latency = np.min(latencies)
    
    print("\n" + "="*30)
    print("ğŸ“Š DINOv3 å•å›¾æœç´¢è€—æ—¶ç»Ÿè®¡")
    print("="*30)
    print(f"å¹³å‡è€—æ—¶ (Average): {avg_latency:.2f} æ¯«ç§’ (ms)")
    print(f"95% åˆ†ä½æ•° (P95):   {p95_latency:.2f} æ¯«ç§’ (ms)")
    print(f"æœ€å¿«è€—æ—¶ (Min):     {min_latency:.2f} æ¯«ç§’ (ms)")
    print(f"æ¯ç§’å¤„ç† (FPS):     {1000/avg_latency:.2f}")
    print("="*30)
    print("æ³¨ï¼šæ­¤è€—æ—¶åŒ…å« [å›¾åƒé¢„å¤„ç† + GPUæ¨ç† + CUDAåŒæ­¥]")

if __name__ == "__main__":
    benchmark()
